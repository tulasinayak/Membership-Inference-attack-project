{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrhgS_swFCKb",
        "outputId": "f9942c4a-54a6-45fb-b0d4-aaf5d77f914d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Shadow Model"
      ],
      "metadata": {
        "id": "TJ68zSdqIVKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "\n",
        "DATA_PATH = '/content/drive/MyDrive/amlm/pickle/cifar10/mobilenetv2/shadow.p'\n",
        "# Change the DATA_PATH to your local pickle file path\n",
        "\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open(DATA_PATH, \"rb\") as f:\n",
        "    dataset = pickle.load(f)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "for batch_idx, (img, label) in enumerate(dataloader):\n",
        "    img = img.to(device)"
      ],
      "metadata": {
        "id": "0hzrVRTuE71M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "29_j5Sp7mkmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "uAtHW1A3mmM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using shadow.p dataset for training shadow model"
      ],
      "metadata": {
        "id": "9EUk96xvIbA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(dataset, test_size=0.36, random_state=42)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_data, transform=transform)\n",
        "test_dataset = CustomDataset(test_data, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=40, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=40, shuffle=False)\n"
      ],
      "metadata": {
        "id": "t_vWF3ramr2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 10)  # 10 classes in CIFAR\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lum_tGCnNFm",
        "outputId": "d54e994b-0d21-41b3-cf10-90b2101f28b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n",
            "100%|██████████| 13.6M/13.6M [00:00<00:00, 172MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
      ],
      "metadata": {
        "id": "K_3mVCGXnO5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcTTTY0QndhS",
        "outputId": "b3ceb49f-55f3-4c26-b268-ebf989340ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "480\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of shadow model"
      ],
      "metadata": {
        "id": "uKA7t7BbgxnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "print(\"Finished Training\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_Npa6Zm5nU02",
        "outputId": "d2bbba83-fc85-4566-f118-7fcd7b67fbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.365428615361452\n",
            "Epoch 2, Loss: 0.9551601548989613\n",
            "Epoch 3, Loss: 0.8036066859960556\n",
            "Epoch 4, Loss: 0.68066303661714\n",
            "Epoch 5, Loss: 0.5830749422932665\n",
            "Epoch 6, Loss: 0.5116408750414848\n",
            "Epoch 7, Loss: 0.44117229074860614\n",
            "Epoch 8, Loss: 0.3809711736937364\n",
            "Epoch 9, Loss: 0.3300514398763577\n",
            "Epoch 10, Loss: 0.2903782879623274\n",
            "Epoch 11, Loss: 0.2765183149836957\n",
            "Epoch 12, Loss: 0.2373372172936797\n",
            "Epoch 13, Loss: 0.20635938873747364\n",
            "Epoch 14, Loss: 0.18670996157452463\n",
            "Epoch 15, Loss: 0.17717469475076844\n",
            "Epoch 16, Loss: 0.15845651519678844\n",
            "Epoch 17, Loss: 0.1482510140844776\n",
            "Epoch 18, Loss: 0.13386676408311662\n",
            "Epoch 19, Loss: 0.13857179802531996\n",
            "Epoch 20, Loss: 0.1198943151345399\n",
            "Epoch 21, Loss: 0.10567103516950738\n",
            "Epoch 22, Loss: 0.11564747688632147\n",
            "Epoch 23, Loss: 0.10444723147763095\n",
            "Epoch 24, Loss: 0.1048007298920614\n",
            "Epoch 25, Loss: 0.10688692073648175\n",
            "Epoch 26, Loss: 0.09269695453112944\n",
            "Epoch 27, Loss: 0.10190789572176678\n",
            "Epoch 28, Loss: 0.08802806548386191\n",
            "Epoch 29, Loss: 0.07887018407297243\n",
            "Epoch 30, Loss: 0.08940175105866123\n",
            "Epoch 31, Loss: 0.08178468277813712\n",
            "Epoch 32, Loss: 0.07823517245681917\n",
            "Epoch 33, Loss: 0.0817677889567373\n",
            "Epoch 34, Loss: 0.08282263666139139\n",
            "Epoch 35, Loss: 0.0680755442820858\n",
            "Epoch 36, Loss: 0.06286175002993938\n",
            "Epoch 37, Loss: 0.0625498518619376\n",
            "Epoch 38, Loss: 0.060955925548114465\n",
            "Epoch 39, Loss: 0.06000280453360271\n",
            "Epoch 40, Loss: 0.07389904357817917\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating shadow model"
      ],
      "metadata": {
        "id": "6WANsuyZg1BI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_o3-1Qi1oZGv",
        "outputId": "72e16481-56d4-433b-9906-eff47748a18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 77.38888888888889%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'amlm_shadow_task1_model.pth')\n"
      ],
      "metadata": {
        "id": "7gsQT0WqQVb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('amlm_shadow_task1_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7_ydlL7zQhRt",
        "outputId": "10312ac8-41c5-4c91-9030-d15b647d9821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_aa42e143-78b8-4bb7-b74b-b8da6b3b583a\", \"amlm_shadow_task1_model.pth\", 9234462)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running the model on train and test dataset, to get confidence for memebers and non-members respectively"
      ],
      "metadata": {
        "id": "Cz-R87HupRlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating attack model dataset"
      ],
      "metadata": {
        "id": "YhIe_j7IaXYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-member confidence"
      ],
      "metadata": {
        "id": "wwrKOtbYg-5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "attack_model_dataset = []\n",
        "cnt = 0\n",
        "with torch.no_grad():\n",
        "    cnt=0\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        for i in range(len(images)):\n",
        "          attack_model_dataset.append(probabilities[i].tolist()+[0]) #0 because it is test data, non-member for the model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GsOJuVvpaFul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(attack_model_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_7prlYUcd1x",
        "outputId": "1b7110e9-ce2f-4032-a73d-021a039ddb8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0003530642716214061, 3.8236740351749177e-07, 3.568402746623178e-07, 3.3166951851626436e-08, 3.37712187103989e-08, 1.2193202891808141e-08, 2.3274431271147478e-08, 9.798864994081669e-06, 0.9996359348297119, 3.811935869180161e-07, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Member confidence"
      ],
      "metadata": {
        "id": "bTUU-1JhhB5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = 0\n",
        "with torch.no_grad():\n",
        "    cnt=0\n",
        "    for images, labels in train_loader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        for i in range(len(images)):\n",
        "          attack_model_dataset.append(probabilities[i].tolist()+[1]) #1 because it is train data, member for the model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h_j_Wdn3bWmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(attack_model_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cPLnCIwcgvW",
        "outputId": "df74e737-6f37-4c90-8812-a41324ca86ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/amlm/attack_model_dataset.pkl','wb') as f:\n",
        "  pickle.dump(attack_model_dataset, f)"
      ],
      "metadata": {
        "id": "A6WWtHurb3ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/amlm/attack_model_dataset.pkl', 'rb') as f:\n",
        "  attack_model_dataset = pickle.load(f)"
      ],
      "metadata": {
        "id": "0I5ag7-fcdLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Attack Model\n",
        "\n",
        "Use confidences achieved to train the model to output 0 or 1 based on membership. Make a binary classifier.\n",
        "\n"
      ],
      "metadata": {
        "id": "-D26_6UzpdbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAttackDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tensor = self.data[idx][:-1]\n",
        "        label = self.data[idx][-1] #last element is the membership, 0 or 1\n",
        "        return torch.tensor(tensor), label\n",
        "\n",
        "\n",
        "attack_dataset = CustomAttackDataset(attack_model_dataset)\n"
      ],
      "metadata": {
        "id": "DkmyJGddjV0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 64)\n",
        "        self.fc2 = nn.Linear(64, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.fc5(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "attack_model = BinaryClassifier()\n"
      ],
      "metadata": {
        "id": "jsDljsogjmh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.90 * len(attack_dataset))\n",
        "test_size = len(attack_dataset) - train_size\n",
        "train_attack_dataset, test_attack_dataset = random_split(attack_dataset, [train_size, test_size])\n",
        "\n",
        "train_attack_loader = DataLoader(train_attack_dataset, batch_size=25, shuffle=True)\n",
        "test_attack_loader = DataLoader(test_attack_dataset, batch_size=25, shuffle=False)\n",
        "\n",
        "\n",
        "print(\"train shape:\", len(train_attack_dataset))\n",
        "\n",
        "print(\"test shape:\", len(test_attack_dataset))\n",
        "#print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87wqKFi1phEq",
        "outputId": "f1f78611-bb3e-48f7-b48e-b2bf0f4bd283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape: 27000\n",
            "test shape: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_attack_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpGJt2NAo2Sd",
        "outputId": "3be25cf4-fe7b-44ca-9ace-1293d299efe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([4.4266e-10, 9.9997e-01, 2.2709e-10, 1.7107e-10, 2.3548e-10, 5.5173e-10,\n",
            "        3.2272e-08, 2.0011e-09, 2.4810e-05, 6.9988e-06]), 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training of attack model"
      ],
      "metadata": {
        "id": "a9_ocypjhV_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(attack_model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "attack_model.to(device)\n",
        "\n",
        "num_epochs = 70\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    attack_model.train()\n",
        "    running_loss = 0.0\n",
        "    cnt = 0\n",
        "    for tensors, labels in train_attack_loader:\n",
        "        cnt += 1\n",
        "        # print(cnt)\n",
        "        tensors, labels = tensors.to(device), labels.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = attack_model(tensors).squeeze()\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_attack_loader)}\")\n",
        "\n",
        "print(\"Finished Training\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0JPVoLdSEzy",
        "outputId": "952fad42-ce65-4c06-bd90-b0a770bf9aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.622252416803881\n",
            "Epoch 2, Loss: 0.5902709840900369\n",
            "Epoch 3, Loss: 0.5856102641809869\n",
            "Epoch 4, Loss: 0.580577376264113\n",
            "Epoch 5, Loss: 0.5783115104944617\n",
            "Epoch 6, Loss: 0.5765505130644197\n",
            "Epoch 7, Loss: 0.5753236063928516\n",
            "Epoch 8, Loss: 0.5740418293685825\n",
            "Epoch 9, Loss: 0.572089725981156\n",
            "Epoch 10, Loss: 0.5717996062227974\n",
            "Epoch 11, Loss: 0.5651165048005404\n",
            "Epoch 12, Loss: 0.5640823460563466\n",
            "Epoch 13, Loss: 0.5637708230702966\n",
            "Epoch 14, Loss: 0.5638546044627826\n",
            "Epoch 15, Loss: 0.5636139520340495\n",
            "Epoch 16, Loss: 0.56305145378466\n",
            "Epoch 17, Loss: 0.5629280604146145\n",
            "Epoch 18, Loss: 0.5629156594751058\n",
            "Epoch 19, Loss: 0.5630569788592833\n",
            "Epoch 20, Loss: 0.5627034647873155\n",
            "Epoch 21, Loss: 0.5620203222665522\n",
            "Epoch 22, Loss: 0.561764219927567\n",
            "Epoch 23, Loss: 0.5617447153561645\n",
            "Epoch 24, Loss: 0.5616911904403457\n",
            "Epoch 25, Loss: 0.5616854160196252\n",
            "Epoch 26, Loss: 0.5616543706644465\n",
            "Epoch 27, Loss: 0.5616226239612809\n",
            "Epoch 28, Loss: 0.5616470712754461\n",
            "Epoch 29, Loss: 0.5615820569296678\n",
            "Epoch 30, Loss: 0.561589227837545\n",
            "Epoch 31, Loss: 0.5614662760937655\n",
            "Epoch 32, Loss: 0.5614624793882723\n",
            "Epoch 33, Loss: 0.561459339686014\n",
            "Epoch 34, Loss: 0.5614568382501602\n",
            "Epoch 35, Loss: 0.5614536103826983\n",
            "Epoch 36, Loss: 0.5614540373837507\n",
            "Epoch 37, Loss: 0.5614515869981713\n",
            "Epoch 38, Loss: 0.561449171381968\n",
            "Epoch 39, Loss: 0.5614469759166241\n",
            "Epoch 40, Loss: 0.5614467791937016\n",
            "Epoch 41, Loss: 0.5614299753473865\n",
            "Epoch 42, Loss: 0.5614299490771911\n",
            "Epoch 43, Loss: 0.5614297047809318\n",
            "Epoch 44, Loss: 0.5614294420789789\n",
            "Epoch 45, Loss: 0.5614293521477116\n",
            "Epoch 46, Loss: 0.5614293497745637\n",
            "Epoch 47, Loss: 0.5614290542624615\n",
            "Epoch 48, Loss: 0.5614289405721206\n",
            "Epoch 49, Loss: 0.5614287505823153\n",
            "Epoch 50, Loss: 0.5614285356744572\n",
            "Epoch 51, Loss: 0.5614268975401366\n",
            "Epoch 52, Loss: 0.5614269071982967\n",
            "Epoch 53, Loss: 0.5614268966846996\n",
            "Epoch 54, Loss: 0.5614268932905462\n",
            "Epoch 55, Loss: 0.5614268841014968\n",
            "Epoch 56, Loss: 0.5614268722909468\n",
            "Epoch 57, Loss: 0.5614268753263686\n",
            "Epoch 58, Loss: 0.5614268621084867\n",
            "Epoch 59, Loss: 0.5614268487802259\n",
            "Epoch 60, Loss: 0.5614268567827013\n",
            "Epoch 61, Loss: 0.5614267833806851\n",
            "Epoch 62, Loss: 0.5614267840153642\n",
            "Epoch 63, Loss: 0.5614267836290378\n",
            "Epoch 64, Loss: 0.5614267837670114\n",
            "Epoch 65, Loss: 0.5614267820285426\n",
            "Epoch 66, Loss: 0.5614267825528427\n",
            "Epoch 67, Loss: 0.5614267845396642\n",
            "Epoch 68, Loss: 0.561426781559432\n",
            "Epoch 69, Loss: 0.5614267815870267\n",
            "Epoch 70, Loss: 0.5614267815870267\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for tensors, labels in test_attack_loader:\n",
        "        tensors, labels = tensors.to(device), labels.to(device).float()\n",
        "        outputs = attack_model(tensors).squeeze()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmpqA93XjB5y",
        "outputId": "56bc037b-7bd1-4e87-cc4f-7bd40bf2470a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 72.46666666666667%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Attack Model using eval.p\n",
        "Input data to target model, get confidence. Input confidence to attack model, receive output. Compare to groundtruth value of membership."
      ],
      "metadata": {
        "id": "15oBWndkpmmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "\n",
        "MODEL_PATH = '/content/drive/MyDrive/amlm/models/mobilenetv2_cifar10.pth'\n",
        "# Change the MODEL_PATH to your local model path\n",
        "\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "target_model = models.mobilenet_v2(num_classes=10).to(device)\n",
        "# Change num_classes to 200 when you use the Tiny ImageNet dataset\n",
        "\n",
        "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "target_model.load_state_dict(state_dict['net'])\n",
        "# Test accuracy\n",
        "acc = state_dict['acc']\n",
        "# Training epoch (start from 0)\n",
        "epoch = state_dict['epoch']"
      ],
      "metadata": {
        "id": "fRZeIHwOFJ_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "EVAL_DATA_PATH = '/content/drive/MyDrive/amlm/pickle/cifar10/mobilenetv2/eval.p'\n",
        "# Change the DATA_PATH to your local pickle file path\n",
        "\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open(EVAL_DATA_PATH, \"rb\") as f:\n",
        "    eval_dataset = pickle.load(f)\n",
        "\n",
        "eval_dataloader = torch.utils.data.DataLoader(\n",
        "    eval_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "for batch_idx, (img, label, membership) in enumerate(eval_dataloader):\n",
        "    img = img.to(device)"
      ],
      "metadata": {
        "id": "bpOG4Gp6tSDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(eval_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RiFh48hy61u",
        "outputId": "6056b7b7-e398-428b-e36c-e795db192177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1L8yGWNr3YRH",
        "outputId": "97ffd835-7336-41ed-8202-bfd60720dd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of target model : 85.0\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "target_model.eval()\n",
        "attack_model_eval_dataset = []\n",
        "total=0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    cnt=0\n",
        "    for images, labels, membership in eval_dataloader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = target_model(images)\n",
        "        #print(outputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for i in range(len(images)):\n",
        "          attack_model_eval_dataset.append(probabilities[i].tolist()+[membership[i]])\n",
        "\n",
        "print(\"Accuracy of target model :\", 100*(correct/total))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(attack_model_eval_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zk6cYPNryxJy",
        "outputId": "4afe59c0-a178-4620-f319-a1470e390bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2022669579891954e-06, 5.424391474662116e-06, 2.5650746465544216e-05, 6.006690819049254e-05, 2.4279757781187072e-05, 0.9998295307159424, 1.5265151887433603e-05, 2.6308618544135243e-05, 7.08294192008907e-06, 5.01111844641855e-06, tensor(1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attack_eval_dataset = CustomAttackDataset(attack_model_eval_dataset)"
      ],
      "metadata": {
        "id": "1bUuoABGzpT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attack_eval_loader = DataLoader(attack_eval_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "uku2MuO_z7hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Finding Accuracy based on eval.p"
      ],
      "metadata": {
        "id": "kS0nNls8a0Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attack_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for tensors, labels in attack_eval_loader:\n",
        "        tensors, labels = tensors.to(device), labels.to(device).float()\n",
        "        outputs = attack_model(tensors).squeeze()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Test Accuracy: {100 * correct / total}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6FNycdq0OAj",
        "outputId": "d4ab1a39-a1f6-4e53-aabe-323d6116ca5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 65.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finding predictions for Test.p"
      ],
      "metadata": {
        "id": "mKygpDm3i6Gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/drive/MyDrive/amlm/pickle/cifar10/mobilenetv2/test.p\"\n",
        "# Change the DATA_PATH to your local pickle file path\n",
        "\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "with open(DATA_PATH, \"rb\") as f:\n",
        "    test_dataset = pickle.load(f)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=64, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "HzzDmW0hwWCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using target model to get confidence"
      ],
      "metadata": {
        "id": "WNkP-33ZjDhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "target_model.eval()\n",
        "final_test_dataset = []\n",
        "total=0\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    cnt=0\n",
        "    for images, labels in test_dataloader:\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = target_model(images)\n",
        "        #print(outputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        for i in range(len(images)):\n",
        "          final_test_dataset.append(probabilities[i].tolist())"
      ],
      "metadata": {
        "id": "0BMd586dwV_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FinalTestDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tensor = self.data[idx]\n",
        "        return torch.tensor(tensor, dtype=torch.float32)\n",
        "\n",
        "final_test_dataset = FinalTestDataset(final_test_dataset)\n",
        "final_test_loader = torch.utils.data.DataLoader(final_test_dataset, batch_size=16, shuffle=False) #Do not shuffle\n"
      ],
      "metadata": {
        "id": "ji18lg_kwVyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using attack model"
      ],
      "metadata": {
        "id": "ExWbFzk4jJtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "attack_model.eval()\n",
        "predicted_values = []\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    for prob in final_test_loader:\n",
        "\n",
        "        prob = prob.to(device)\n",
        "        outputs = attack_model(prob).squeeze()\n",
        "        predicted = (outputs > 0.5).float()\n",
        "        predicted_values.extend(predicted.cpu().numpy())\n",
        "\n",
        "# List of predictions to a NumPy array\n",
        "predicted_array = np.array(predicted_values)\n",
        "\n",
        "# Printing the NumPy array\n",
        "print(predicted_array)\n",
        "\n",
        "# Saving NumPy array to a file\n",
        "np.save('predicted_values.npy', predicted_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYa6kyhKwb2K",
        "outputId": "b709a9d4-699b-4805-9017-c91461c239bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 1. ... 1. 1. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def compare_npy_files(file1, file2):\n",
        "    # Load arrays from the files\n",
        "    arr1 = np.load(file1)\n",
        "    arr2 = np.load(file2)\n",
        "\n",
        "    # Compare the arrays\n",
        "    if np.array_equal(arr1, arr2):\n",
        "        print(\"Arrays are identical.\")\n",
        "    else:\n",
        "        print(\"Arrays are different.\")\n",
        "\n",
        "# Paths to the .npy files\n",
        "file1_path = \"/content/latest_predictions.npy\"\n",
        "file2_path = \"/task1_mobilenetv2_cifar10.npy\"\n",
        "\n",
        "# Compare the files\n",
        "compare_npy_files(file1_path, file2_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAG6VR8oJGsO",
        "outputId": "6d9db64f-61fa-4333-e8fb-bb6eec580ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrays are different.\n"
          ]
        }
      ]
    }
  ]
}